---
title: "IML-set1"
author: "KS"
date: "2025-01-29"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries}
library(tidyverse)
library(caret)
library(boot)
library(randomForest)
library(e1071)
```

## Problem 1

### Learning Objective: familiarity with tools, basic description of the data set, familiarisation with the term project data

```{r}
# Data for this problem is a subset of the original data.
# The specific subsetting criteria are not being disclosed for confidentiality reasons.
# The subsetted data is loaded from a local file (not included in the repo).
# Same applies for kaggle data at the end of this problem
my_data <- read_csv("p1.csv")
```

```{r}
# Inspect the data to understand its structure and variables.
# This helps in planning the subsequent analysis steps.
head(my_data)
glimpse(my_data)
summary(my_data)
```

```{r}
# Remove ID, SMILES, and InChIKey columns. These columns are unique 
# identifiers for each molecule and are not relevant for predicting the 
# log10_pSat_Pa property. They do not contain information about the 
# molecular structure or properties that are used in the predictive models.  
# Including them would not improve model performance and could potentially 
# introduce noise or bias.
my_data <- my_data %>% select(-id, -SMILES, -InChIKey)
```

```{r}
# Verify that the columns have been dropped.
head(my_data)
```

```{r}
# Select specific columns for further analysis and summary statistics.
# We focus on 'log10_pSat_Pa' (the target variable), 'NumOfConf' 
# (number of conformers), and 'ChemPot_kJmol' (chemical potential) as 
# these are likely to be important molecular properties.
selected_data <- my_data %>% select(log10_pSat_Pa, NumOfConf, ChemPot_kJmol)
```

```{r}
# Print summary statistics for the selected variables.
# This provides insights into the distributions of these variables 
# (mean, median, quartiles, etc.).
summary(selected_data)

```

```{r}
# Extract the 'ChemPot_kJmol' column as an array for further calculations.
# This is done to calculate the mean and standard deviation of this 
# specific property.
ChemPot_array <- my_data$ChemPot_kJmol
```

```{r}
# Calculate and print the mean and standard deviation of the 'ChemPot_kJmol' array.
# These statistics describe the central tendency and spread of this variable.
mean(ChemPot_array)
sd(ChemPot_array)
```

```{r}
# Produce side-by-side plots to visualize the distributions of 
# 'log10_pSat_Pa' and 'NumOfConf'.
# The histogram of 'log10_pSat_Pa' helps us understand its distribution 
# (e.g., normality, skewness).
# The boxplot of 'NumOfConf' helps us identify potential outliers and 
# understand its spread.
par(mfrow=c(1,2))
hist(my_data$log10_pSat_Pa, 
     xlab = "log10_pSat_Sa", 
     main = "Distribution of log10_pSat_Pa")
boxplot(my_data$NumOfConf, 
        ylab = "NumOfConf", 
        main = "Distribution of NumOfConf")

```

```{r}
# Produce a scatter-plot matrix of 'MW' (molecular weight), 
# 'HeatOfVap_kJmol' (heat of vaporization), and 'FreeEnergy_kJmol' 
# (free energy). This allows us to visually assess potential correlations
# between these molecular properties.
pairs(my_data[, c("MW", "HeatOfVap_kJmol", "FreeEnergy_kJmol")], 
      main = "Scatterplot Matrix of MW, HeatOfVap_kJmol, and FreeEnergy_kJmol")
```

```{r}
# Load the test and training datasets from the Kaggle competition.
kaggle_test = read_csv("test_kaggle.csv")
kaggle_train = read_csv("train_kaggle.csv")
```

```{r}
# Extract test set IDs to prepare the submission file.
kaggle_pred <- kaggle_test['ID']
```

```{r}
# Create a dummy model that predicts the mean of the training response 
# variable ('log_pSat_Pa').
# This serves as a baseline model to compare against more complex models.  
# It helps us understand how much better (or worse) our models are performing 
# compared to a very simple prediction.
kaggle_pred$TARGET <- mean(kaggle_train$log_pSat_Pa)
```

```{r}
# Write the predictions to a CSV file for submission to the Kaggle competition.
write.csv(kaggle_pred, "kaggle_pred.csv", row.names = FALSE)
```

```{r}
# Note: The Kaggle score for this dummy model is expected to be close to zero 
# (or even slightly negative) because it's simply predicting the mean and not 
# taking into account any of the features.  It serves as a baseline for 
# comparison.
# Kaggle score: -0.0001
```

## Problem 2

### Learning objective: learning linear regression, concrete use of validation set, k-fold cross-validation, using regression models from ML libraries, and generalisation

```{r}
# Load the training, validation, and test datasets.
train_syn <- read_csv("data/train_syn.csv")
valid_syn <- read_csv("data/valid_syn.csv")
test_syn <- read_csv("data/test_syn.csv")

# Combine the training and validation sets.  This combined dataset will be used
# for cross-validation to estimate the model's performance on unseen data.
combined_syn <- rbind(train_syn, valid_syn)

```

#### Task a

```{r}
# Initialize a data frame to store the results (errors for different polynomial degrees).
results <- data.frame(
  Degree = numeric(0),
  Train = numeric(0),
  Validation = numeric(0),
  Test = numeric(0),
  TestTRVA = numeric(0), # Test error for model trained on Train + Validation
  CV = numeric(0)       # Cross-validation error
)


```

```{r}
# Initialize a list to store the fitted models.  This allows us to access
# the models later for plotting or other analysis.
models <- list()

```

```{r}
# Loop through polynomial degrees from 0 to 8.
for (degree in 0:8) {
  # Fit the polynomial models.
  # For degree 0, we fit a simple linear model with only an intercept (no x term).
  # For degrees 1 and higher, we use the poly() function to create polynomial terms.
  if (degree == 0) {
    model_degree <- lm(y ~ 1, data = train_syn)       # Model trained on training set
    model_comb_degree <- glm(y ~ 1, data = combined_syn) # Model trained on combined set
  } else {
    model_degree <- lm(y ~ poly(x, degree), data = train_syn)       # Model trained on training set
    model_comb_degree <- glm(y ~ poly(x, degree), data = combined_syn) # Model trained on combined set
  }

  # Store the fitted models in the list.
  models[[paste0("model_", degree)]] <- model_degree
  models[[paste0("model_comb_", degree)]] <- model_comb_degree

  # Calculate the mean squared errors (MSE) on the different sets.
  train_error <- mean((train_syn$y - predict(model_degree))^2)
  validation_error <- mean((valid_syn$y - predict(model_degree))^2)
  test_error <- mean((test_syn$y - predict(model_degree))^2)
  test_trva_error <- mean((test_syn$y - predict(model_comb_degree))^2)

  # Perform 5-fold cross-validation on the combined training and validation set.
  # This provides a more robust estimate of the model's performance on unseen data.
  cv_error <- cv.glm(combined_syn, model_comb_degree, K = 5)$delta[1] # Extract the CV error

  # Add the results for the current degree to the results data frame.
  results <- rbind(results, data.frame(
    Degree = degree,
    Train = train_error,
    Validation = validation_error,
    Test = test_error,
    TestTRVA = test_trva_error,
    CV = cv_error
  ))
}
```

```{r}
# Print the table of results.
results

```

```{r}
# Based on the training and validation losses, we choose a polynomial degree
# between 2 and 5.  These degrees offer a good balance between minimizing
# both training and validation error.  Lower degrees might underfit the data,
# while higher degrees might overfit.
# Further analysis (looking at test error and cross-validation error) is needed
# to make a final decision.

```

#### Task b

```{r}
# Plot the fitted polynomial curves for selected degrees.
# We use the models trained on the training set for these plots.

# Create x values for plotting the curves smoothly.
x_plot <- seq(from = -3, to = 3, length.out = 256)
```

```{r}
# Loop through the selected degrees.
for (degree in c(0, 1, 2, 3, 4, 8)) {
  # Get the model from the list.
  model_name <- paste0("model_", degree)
  model <- models[[model_name]]

  # Create the plot.
  plot(train_syn$x, train_syn$y, 
       xlab = "x", ylab = "y", 
       main = paste("Degree", degree), # Clear title indicating the degree
       xlim = c(-3, 3),              # Set x-axis limits for consistency
       ylim = c(min(train_syn$y), max(train_syn$y)) # Set y-axis limits for consistency
  )

  # Add the fitted polynomial curve in red.
  lines(x_plot, predict(model, newdata = data.frame(x = x_plot)), col = "red")
}

```

#### Task c

```{r}
# Load the real-world training and test data.
real_train <- read_csv("train_real.csv")
real_test <- read_csv("test_real.csv")
```

```{r}
# The response variable is 'Next_Tmax' (next day's maximum temperature).

# Calculate the mean of 'Next_Tmax' in the training set.
mean_next_tmax <- mean(real_train$Next_Tmax)
```

```{r}
# Create a dummy model that always predicts the mean 'Next_Tmax'.
# This serves as a baseline for comparison.
dummy_model <- lm(Next_Tmax ~ 1, data = real_train)  # simpler way for dummy model
dummy_pred <- predict(dummy_model, newdata = real_test)
```

```{r}
# Fit an ordinary least squares (OLS) linear regression model.
# We use all predictors except 'station'.
ols_model <- lm(Next_Tmax ~ . -station, data = real_train)
ols_pred <- predict(ols_model, newdata = real_test)

# Fit a random forest model.
rf_model <- randomForest(Next_Tmax ~ . -station, data = real_train)
rf_pred <- predict(rf_model, newdata = real_test)

# Fit a support vector regression (SVR) model.
svr_model <- svm(Next_Tmax ~ . - station, data = real_train)
svr_pred <- predict(svr_model, newdata = real_test)

# Fit a k-nearest neighbors (KNN) regression model.
knn_model <- knnreg(Next_Tmax ~ . - station, data = real_train)
knn_pred <- predict(knn_model, newdata = real_test)
```

```{r}
# Calculate the Root Mean Squared Error (RMSE) for each model on both the
# training and test sets.  RMSE is a common metric for evaluating regression models.
rmse_dummy_test <- RMSE(dummy_pred, real_test$Next_Tmax)
rmse_ols_test <- RMSE(ols_pred, real_test$Next_Tmax)
rmse_rf_test <- RMSE(rf_pred, real_test$Next_Tmax)
rmse_svr_test <- RMSE(svr_pred, real_test$Next_Tmax)
rmse_knn_test <- RMSE(knn_pred, real_test$Next_Tmax)
```

```{r}
rmse_dummy_train <- RMSE(predict(dummy_model, newdata = real_train), real_train$Next_Tmax)
rmse_ols_train <- RMSE(predict(ols_model, newdata = real_train), real_train$Next_Tmax)
rmse_rf_train <- RMSE(predict(rf_model, newdata = real_train), real_train$Next_Tmax)
rmse_svr_train <- RMSE(predict(svr_model, newdata = real_train), real_train$Next_Tmax)
rmse_knn_train <- RMSE(predict(knn_model, newdata = real_train), real_train$Next_Tmax)
```

```{r, include=FALSE}
# Specify 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Formula (using all predictors except 'station')
formula <- Next_Tmax ~ . - station

# Calculate CV RMSE for each model using the specified control
cv_dummy <- train(formula, data = real_train, method = "lm", trControl = ctrl)
cv_ols <- train(formula, data = real_train, method = "lm", trControl = ctrl)
cv_rf <- train(formula, data = real_train, method = "rf", trControl = ctrl)
cv_svr <- train(formula, data = real_train, method = "svmRadial", trControl = ctrl)
cv_knn <- train(formula, data = real_train, method = "knn", trControl = ctrl)

# Extract and print CV RMSE values (using the RMSE corresponding to the lowest MAE)
print(paste("Dummy CV RMSE:", cv_dummy$results$RMSE[which.min(cv_dummy$results$MAE)]))
print(paste("OLS CV RMSE:", cv_ols$results$RMSE[which.min(cv_ols$results$MAE)]))
print(paste("RF CV RMSE:", cv_rf$results$RMSE[which.min(cv_rf$results$MAE)]))
print(paste("SVR CV RMSE:", cv_svr$results$RMSE[which.min(cv_svr$results$MAE)]))
print(paste("KNN CV RMSE:", cv_knn$results$RMSE[which.min(cv_knn$results$MAE)]))
```

```{r, echo=FALSE}
# Create a data frame to store the RMSE values
rmse_table <- data.frame(
  Model = c("Dummy", "OLS", "RF", "SVR", "KNN"),
  Train_RMSE = c(rmse_dummy_train, rmse_ols_train, rmse_rf_train, rmse_svr_train, rmse_knn_train),
  Test_RMSE = c(rmse_dummy_test, rmse_ols_test, rmse_rf_test, rmse_svr_test, rmse_knn_test),
  CV_RMSE = c(cv_dummy$results$RMSE[which.min(cv_dummy$results$MAE)], 
              cv_ols$results$RMSE[which.min(cv_ols$results$MAE)],
              cv_rf$results$RMSE[which.min(cv_rf$results$MAE)],
              cv_svr$results$RMSE[which.min(cv_svr$results$MAE)],
              cv_knn$results$RMSE[which.min(cv_knn$results$MAE)])
)

# Print the table
print(rmse_table)
```

Based on this table I would say RF is the best regressor because it has the lowest TEST_RMSE and CV_RMSE is pretty close to TEST_RMSE which indicates that it is somewhat robust as well on unseen data.

Train seems to be much lower than test (dummy being exception). CV seem to be much closer to test (dummy being exception).

Since I used all other explanatory variables than station, I could improve these regressors with this data by feature engineering.

## Problem 3

### Learning objectives: bias and variance and model flexibility

#### Task a

Training error usually tends to decrease when we move from less flexible into more flexible models. Testing error on the other hand forms a U shaped graph. This is because too flexible model is over fitting to the training data.

Bias usually decreases when moving from less flexible models towards more flexible models. This is because the more complex model is fitting super perfectly to the training data.

Variance on the other hand tends to increase the more flexible model is.

Irreducible error is irreducible error in any model and this can not be explained by any model.

#### Task b

Data generating process is described as follows f(x) = 3 + x - 2x\^2 + epsilon, epsilon \~ N(0, 0.4\^2)

#### i

```{r}
# Define the data generating function
generate_data <- function(set_size) {
  x <- runif(set_size, -3, 3)  # Generate x values between -3 and 3
  epsilon <- rnorm(set_size, mean = 0, sd = 0.4)  # Generate error terms
  y <- 3 + x - 2 * x^2 + epsilon  # Calculate y values
  return(data.frame(x = x, y = y))
}
```

```{r}
# Number of training sets
number_sets <- 1000
set_size <- 10
```

```{r}
# Generate the training sets
training_sets <- replicate(number_sets, generate_data(set_size), simplify = FALSE)

```

```{r}
# Function to calculate metrics for a given degree and set
calculate_metrics <- function(degree, set) {
  if (degree == 0) {
    model <- lm(y ~ 1, data = set)
  } else {
    model <- lm(y ~ poly(x, degree), data = set)
  }
  
  f_hat_0 <- predict(model, newdata = data.frame(x = 0))
  f_0 <- 3  # True f(0)
  y_0 <- generate_data(1)$y # Generate a new y value at x=0 (Important!)

  mse_0 <- (f_hat_0 - y_0)^2
  return(list(f_hat = f_hat_0, f = f_0, y = y_0, mse = mse_0))
}

```

```{r}
# Initialize results table
degree_table <- data.frame(
  Degree = integer(),
  Irreducible = numeric(),
  BiasSq = numeric(),
  Variance = numeric(),
  Total = numeric(),
  MSE = numeric()
)
```

```{r}
# Loop through degrees 0 to 6
for (degree in 0:6) {
  results <- data.frame(f_hat = numeric(0), f = numeric(0), y = numeric(0), mse = numeric(0))
  
  for (set in training_sets) {
    metrics <- calculate_metrics(degree, set)
    results <- rbind(results, data.frame(f_hat = metrics$f_hat, f = metrics$f, y = metrics$y, mse = metrics$mse))
  }
  
  means <- colMeans(results)
  
  irr <- var(results$y)  # Irreducible error is the variance of the true y values
  biassq <- (means["f_hat"] - 3)^2  # Bias squared
  var <- var(results$f_hat)  # Variance of the predictions
  tot <- irr + biassq + var  # Total error
  mse <- means["mse"] # Mean Squared Error

  degree_table <- rbind(degree_table, data.frame(
    Degree = degree,
    Irreducible = irr,
    BiasSq = biassq,
    Variance = var,
    Total = tot,
    MSE = mse
  ))
}

```

```{r}
print(degree_table)
```

#### ii

```{r}
ggplot(degree_table, aes(x = Degree)) + 
  geom_line(aes(y = Irreducible, color = "Irreducible")) + 
  geom_line(aes(y = BiasSq, color = "BiasSq")) + 
  geom_line(aes(y = Variance, color = "Variance")) + 
  geom_line(aes(y = Total, color = "Total")) +
  labs(x = "Degree", y = "Value", title = "Irreducible, BiasSq, Variance, and Total vs. Degree") +
  scale_color_manual(values = c("Irreducible" = "red", "BiasSq" = "blue", "Variance" = "green", "Total" = "black")) +
  theme_bw()

```

#### iii

These terms behave as expected. When model flexibility increases bias is coming down and variance is going up. At one point (in the middle) variance and bias goes past each others as expected. At the same time Total seems to calculate the sum of other three flawlessly.

## Problem 5

### Learning Objective: properties of estimators

#### Task a

```{r}
# Read data for models
df5_d1 <- read_csv("data/d1.csv")
df5_d2 <- read_csv("data/d2.csv")
df5_d3 <- read_csv("data/d3.csv")
df5_d4 <- read_csv("data/d4.csv")

```

```{r}
# Build OLS linear regression models for each dataset
modeld1 <- lm(y ~ x, data = df5_d1)
modeld2 <- lm(y ~ x, data = df5_d2)
modeld3 <- lm(y ~ x, data = df5_d3)
modeld4 <- lm(y ~ x, data = df5_d4)
```

```{r}
# Function to print and interpret model summary
print_and_interpret <- function(model, model_name) {
  print(summary(model))
  cat("\nInterpretation for", model_name, ":\n")

  intercept <- coef(model)[1]
  slope <- coef(model)[2]
  p_value_intercept <- summary(model)$coefficients[1, 4]
  p_value_slope <- summary(model)$coefficients[2, 4]
  r_squared <- summary(model)$r.squared

  cat("Intercept:", intercept, ", p-value:", p_value_intercept, "\n")
  cat("Slope:", slope, ", p-value:", p_value_slope, "\n")
  cat("R-squared:", r_squared, "\n")

  # Example interpretation (adjust significance level as needed)
  if (p_value_slope < 0.01) {
    cat("The slope is statistically significant at the 0.01 level.\n")
    cat("This suggests that there is a relationship between x and y.\n")
    cat("For every one unit increase in x, y is expected to change by", slope, "units.\n")
  } else {
    cat("The slope is not statistically significant at the 0.01 level.\n")
  }
  cat("R-squared indicates that the model explains", r_squared * 100, "% of the variance in y.\n\n")
}

```

```{r}
# Print and interpret summaries
print_and_interpret(modeld1, "Model 1")
print_and_interpret(modeld2, "Model 2")
print_and_interpret(modeld3, "Model 3")
print_and_interpret(modeld4, "Model 4")
```

#### Task b

```{r}
plot_model <- function(data, model, title) {
  ggplot(data, aes(x = x, y = y)) +
    geom_point() +
    geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = "red") + # Use abline for OLS
    labs(x = "x", y = "y", title = title) +
    theme_bw()
}
```

```{r}
plot_model(df5_d1, modeld1, "OLS Model 1")
plot_model(df5_d2, modeld2, "OLS Model 2")
plot_model(df5_d3, modeld3, "OLS Model 3")
plot_model(df5_d4, modeld4, "OLS Model 4")
```

From the plot I can see that these data sets are crafted quite cleverly so that each set produces roughly the same OLS model even thou one could say that each of these data sets are from different distribution clearly.

So in super short: models the same, data sets different.

#### Task c

Six potential problems with linear regression models mentioned in ISLR:

1.  **Non-linearity of the response variable:** The relationship between the predictor (x) and the response (y) might not be linear. This can be seen by patterns in residual plots or by comparing models with different functional forms (e.g., polynomial vs. linear).

2.  **Correlation of error terms:** The errors in the model might be correlated with each other. This violates a key assumption of linear regression and can lead to incorrect standard errors and p-values. Time series data is particularly susceptible to this.

3.  **Non-constant variance of error terms (Heteroscedasticity):** The variance of the errors might not be constant across all levels of the predictor. This can also lead to incorrect standard errors and p-values. Residual plots can help diagnose this.

4.  **Outliers:** Outliers are observations that are unusually far away from the other data points. They can have a large influence on the regression line and can distort the model.

5.  **High-leverage points:** High-leverage points are observations that have unusual predictor values. They can also have a large influence on the regression line, even if they are not outliers in the response variable.

6.  **Collinearity:** Collinearity occurs when two or more predictors in a multiple regression model are highly correlated with each other. This can make it difficult to determine the individual effects of each predictor on the response. (Not applicable in this case with only one predictor.)

```{r}
# Example: Non-linearity (as before - showing how to check and fix)
plot(modeld2, which = 1) # Check for non-linearity (residual plot)

modeld2_trans <- lm(y ~ poly(x, 2), data = df5_d2) # Fit a polynomial model
plot(modeld2_trans, which = 1) # Check the residuals again (should be better)

summary(modeld2) # Check R-squared (how much variance explained)
summary(modeld2_trans) # Compare with the polynomial model R-squared
```

## Problem 6

### Learning Objective: properties of estimators and Bootstrap

Let's copy from Problem 5 standard error for intercept and slope for model 2:

Modeld2: Std. Error = 1.125 Modeld2: Std. Error = 0.118

```{r, include=FALSE}
# Function to perform a single bootstrap iteration
bootstrap_iteration <- function(data) {
  # Sample with replacement
  boot_sample <- data[sample(nrow(data), replace = TRUE), ]
  # Fit the model
  model <- lm(y ~ x, data = boot_sample)
  # Return the coefficients
  return(coef(model))
}

# Set the number of bootstrap iterations
n_iterations <- 1000 

# Perform the bootstrap
bootstrap_coefs <- replicate(n_iterations, bootstrap_iteration(df5_d2))

# Calculate the standard errors
standard_errors <- apply(bootstrap_coefs, 1, sd)

```

```{r}
# Compare bootstrap SEs to model-based SEs
cat("Model-based Standard Errors:\n")
cat("Intercept:", summary(modeld2)$coefficients[1, 2], "\n")
cat("Slope:", summary(modeld2)$coefficients[2, 2], "\n\n")

cat("Bootstrap Standard Errors:\n")
cat("Intercept:", standard_errors[1], "\n")
cat("Slope:", standard_errors[2], "\n\n")

# Discuss the differences
cat("Discussion:\n")
cat("The bootstrap standard errors are slightly higher than the model-based standard errors.\n")
cat("This is expected, as the bootstrap provides a more robust estimate, especially given the small sample size and potential issues with the linear model assumptions (constant variance, independent errors) in this dataset.\n")
cat("The bootstrap approach does not rely on these assumptions and provides a distribution of possible coefficient values, from which the standard error is calculated.  It's a more data-driven approach.\n")
cat("In this case, the bootstrap SEs likely reflect the greater uncertainty due to the small sample size and potential model misspecification.\n")

```

So the bootsrtap samples my data with replacement and fits a new model for that. It repeats this say 1000 times and collects my intercept and slope coefficients.

After this iteration it calculates standard error for my parameters from this collection.

These would be my bootstrap estimators for standard error for intercept and slope term in question.

#### Task c

If we pull say 4 at n:th pull, we ask what is the probability that we have not pulled 4 before. This can be written as ((n-1)/n)\^n to demonstrate that we dodged 4 with every pull so far. We can plot this function as follows with a dashed line at 1/e:

```{r}
curve(((x-1)/x)^x, from = 1, to = 1000, 
      xlab = "n", ylab = "((n-1)/n)^n", main = "Plot of ((n-1)/n)^n",
      ylim = c(0, 0.5))
abline(h = 1/exp(1), lty = 2)
```

From the plot we can see that ((n-1)/n) converges to 1/e as n approaches infinity.
